{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single model performance - ProBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "06/04/2019 17:50:36 - INFO - neuralcoref -   Loading model from /home/sandeep_attree/.neuralcoref_cache/neuralcoref\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0604 17:50:36.330217 140448954050304 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "                  id            text pronoun  ...  b_offset b_coref             url\n",
      "0      development-1  Zoe Telford...     her  ...       207   False  http://en.w...\n",
      "1      development-2  He grew up ...     His  ...       251   False  http://en.w...\n",
      "2      development-3  He had been...     his  ...       246    True  http://en.w...\n",
      "3      development-4  The current...     his  ...       336    True  http://en.w...\n",
      "4      development-5  Her Santa F...     She  ...       294    True  http://en.w...\n",
      "...              ...             ...     ...  ...       ...     ...             ...\n",
      "1995  development...  Faye's thir...     her  ...       328    True  http://en.w...\n",
      "1996  development...  The plot of...     her  ...       215    True  http://en.w...\n",
      "1997  development...  Grant playe...     she  ...       266   False  http://en.w...\n",
      "1998  development...  The fashion...     She  ...       208   False  http://en.w...\n",
      "1999  development...  Watkins was...     her  ...       347    True  http://en.w...\n",
      "\n",
      "[2000 rows x 11 columns]\n",
      "                 id            text pronoun  ...  b_offset b_coref             url\n",
      "0      validation-1  He admitted...     him  ...       241   False  http://en.w...\n",
      "1      validation-2  Kathleen No...     She  ...       150    True  http://en.w...\n",
      "2      validation-3  When she re...     his  ...       406    True  http://en.w...\n",
      "3      validation-4  On 19 March...      he  ...       325   False  http://en.w...\n",
      "4      validation-5  By this tim...     she  ...       328    True  http://en.w...\n",
      "..              ...             ...     ...  ...       ...     ...             ...\n",
      "449  validation-450  He then agr...      He  ...       264   False  http://en.w...\n",
      "450  validation-451  Disgusted w...     she  ...       257   False  http://en.w...\n",
      "451  validation-452  She manipul...     she  ...       291    True  http://en.w...\n",
      "452  validation-453  On April 4,...     her  ...       294    True  http://en.w...\n",
      "453  validation-454  Pleasant ex...     him  ...       255   False  http://en.w...\n",
      "\n",
      "[454 rows x 11 columns]\n",
      "             id            text pronoun  ...  b_offset b_coref             url\n",
      "0        test-1  Upon their ...     His  ...       366    True  http://en.w...\n",
      "1        test-2  Between the...     him  ...       390   False  http://en.w...\n",
      "2        test-3  Though his ...      He  ...       295   False  http://en.w...\n",
      "3        test-4  At the tria...     his  ...       536    True  http://en.w...\n",
      "4        test-5  It is about...     his  ...       559   False  http://en.w...\n",
      "...         ...             ...     ...  ...       ...     ...             ...\n",
      "1995  test-1996  The sole ex...     She  ...       432   False  http://en.w...\n",
      "1996  test-1997  According t...     her  ...       404   False  http://en.w...\n",
      "1997  test-1998  In June 200...     She  ...       412   False  http://en.w...\n",
      "1998  test-1999  She was del...     she  ...       274   False  http://en.w...\n",
      "1999  test-2000  Meg and Vic...     her  ...       260   False  http://en.w...\n",
      "\n",
      "[2000 rows x 11 columns]\n",
      "            id            text pronoun  ...  b_offset b_coref             url\n",
      "0      test-16  Indeed, Bus...     his  ...       122   False  http://en.w...\n",
      "1      test-19  On May 17, ...     her  ...       293   False  http://en.w...\n",
      "2      test-24  Jones playe...      He  ...       319   False  http://en.w...\n",
      "3      test-41  However he ...     his  ...       213   False  http://en.w...\n",
      "4      test-46  When Liu Be...      he  ...       136   False  http://en.w...\n",
      "..         ...             ...     ...  ...       ...     ...             ...\n",
      "248  test-1968  Laura was t...     She  ...       189   False  http://en.w...\n",
      "249  test-1975  In the seve...     her  ...       209   False  http://en.w...\n",
      "250  test-1977  She also pl...     She  ...       105   False  http://en.w...\n",
      "251  test-1980  After Richa...     she  ...       392   False  http://en.w...\n",
      "252  test-1993  In a draft ...     her  ...       446   False  http://en.w...\n",
      "\n",
      "[253 rows x 11 columns]\n",
      "Initializing Step InputReader\n",
      "Step InputReader initialized\n",
      "Initializing Step LabelSanitizer\n",
      "Step LabelSanitizer initialized\n",
      "Initializing Step MentionsAnnotator\n",
      "Step MentionsAnnotator initialized\n",
      "Initializing Step gather_step\n",
      "STEPPY WARNING: Step with name \"InputReader\", already exist. Make sure that all Steps have unique name.\n",
      "Step gather_step initialized\n",
      "Initializing Step InputReader\n",
      "Step InputReader initialized\n",
      "Initializing Step LabelSanitizer\n",
      "Step LabelSanitizer initialized\n",
      "Initializing Step MentionsAnnotator\n",
      "Step MentionsAnnotator initialized\n",
      "Initializing Step gather_step\n",
      "STEPPY WARNING: Step with name \"InputReader\", already exist. Make sure that all Steps have unique name.\n",
      "Step gather_step initialized\n",
      "Initializing Step InputReader\n",
      "Step InputReader initialized\n",
      "Initializing Step LabelSanitizer\n",
      "Step LabelSanitizer initialized\n",
      "Initializing Step MentionsAnnotator\n",
      "Step MentionsAnnotator initialized\n",
      "Initializing Step gather_step\n",
      "STEPPY WARNING: Step with name \"InputReader\", already exist. Make sure that all Steps have unique name.\n",
      "Step gather_step initialized\n",
      "Initializing Step InputReader\n",
      "Step InputReader initialized\n",
      "Initializing Step LabelSanitizer\n",
      "Step LabelSanitizer initialized\n",
      "Initializing Step MentionsAnnotator\n",
      "Step MentionsAnnotator initialized\n",
      "Initializing Step gather_step\n",
      "STEPPY WARNING: Step with name \"InputReader\", already exist. Make sure that all Steps have unique name.\n",
      "Step gather_step initialized\n",
      "{'edges': {('InputReader', 'MentionsAnnotator'),\n",
      "           ('InputReader', 'gather_step'),\n",
      "           ('LabelSanitizer', 'gather_step'),\n",
      "           ('MentionsAnnotator', 'gather_step'),\n",
      "           ('input', 'InputReader'),\n",
      "           ('input', 'LabelSanitizer')},\n",
      " 'nodes': {'InputReader',\n",
      "           'LabelSanitizer',\n",
      "           'MentionsAnnotator',\n",
      "           'gather_step',\n",
      "           'input'}}\n",
      "Transforming data to features.\n",
      "Step gather_step, working in \"train\" mode\n",
      "Step InputReader, working in \"train\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step LabelSanitizer, working in \"train\" mode\n",
      "Step LabelSanitizer, adapting inputs\n",
      "Step LabelSanitizer, transforming...\n",
      "      id                                            label\n",
      "0     14                                       Gracia (B)\n",
      "1     35                                 Lim Goh Tong (B)\n",
      "2     40                                          neither\n",
      "3    104                                Polly Simmons (B)\n",
      "4    120  neither (differentiate between person and role)\n",
      "5    140                                       Isabel (B)\n",
      "6    170                                          Day (B)\n",
      "7    171                                              (A)\n",
      "8    259                                              (B)\n",
      "9    260                                        Smith (A)\n",
      "10   290                                     Lewellyn (A)\n",
      "11   324                                   Umfraville (B)\n",
      "12   327                                          neither\n",
      "13   336                                          Rex (B)\n",
      "14   345                         neither (person vs role)\n",
      "15   347                                          neither\n",
      "16   376                                              (A)\n",
      "17   400                                              (A)\n",
      "18   429                                          neither\n",
      "19   443                                          neither\n",
      "20   456                                    Dan Brown (A)\n",
      "21   467                                          neither\n",
      "22   473                     neither (actor vs character)\n",
      "23   483                                       Huang (B) \n",
      "24   527                                     Cynthia (B) \n",
      "25   540                                Frances Black (B)\n",
      "26   547                                          neither\n",
      "27   565                                       Ragnar (A)\n",
      "28   577                                              (A)\n",
      "29   586                                  Leadbetter (A) \n",
      "..   ...                                              ...\n",
      "43  1050                                          neither\n",
      "44  1085                                          neither\n",
      "45  1160                                              (B)\n",
      "46  1191                                              (A)\n",
      "47  1203                                              (B)\n",
      "48  1215                                              (A)\n",
      "49  1258                                          neither\n",
      "50  1291                                          neither\n",
      "51  1337        (B)  (let’s see what the model does here)\n",
      "52  1368                                              (A)\n",
      "53  1415                                              (B)\n",
      "54  1421                                              (A)\n",
      "55  1454                                              (A)\n",
      "56  1484                                              (B)\n",
      "57  1512                                          neither\n",
      "58  1564                                              (B)\n",
      "59  1568                     neither (actor vs character)\n",
      "60  1569                     neither (actor vs character)\n",
      "61  1572                 neither (or A, definitely not B)\n",
      "62  1614                                              (A)\n",
      "63  1653                                              (B)\n",
      "64  1679                                              (B)\n",
      "65  1709                                              (B)\n",
      "66  1721                     neither (actor vs character)\n",
      "67  1744                                              (B)\n",
      "68  1793                                              (A)\n",
      "69  1804                     neither (actor vs character)\n",
      "70  1873                                              (A)\n",
      "71  1933                                              (A)\n",
      "72  1993                                              (B)\n",
      "\n",
      "[73 rows x 2 columns]\n",
      "Step LabelSanitizer, transforming completed\n",
      "Step LabelSanitizer, transform completed\n",
      "Step MentionsAnnotator, working in \"train\" mode\n",
      "Step InputReader, working in \"train\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step MentionsAnnotator, unpacking inputs\n",
      "Step MentionsAnnotator, transforming...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying..: 100%|██████████████████████████| 2000/2000 [00:02<00:00, 726.73it/s]\n",
      "Step MentionsAnnotator, transforming completed\n",
      "Step MentionsAnnotator, transform completed\n",
      "Step gather_step, adapting inputs\n",
      "Step gather_step, transforming...\n",
      "Step gather_step, transforming completed\n",
      "Step gather_step, transform completed\n",
      "Step gather_step, working in \"val\" mode\n",
      "Step InputReader, working in \"val\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step LabelSanitizer, working in \"val\" mode\n",
      "Step LabelSanitizer, adapting inputs\n",
      "Step LabelSanitizer, transforming...\n",
      "     id                         label\n",
      "0    29                        either\n",
      "1    79                   Lillian (B)\n",
      "2    80              Edward Jones (A)\n",
      "3   111               Jane Austen (A)\n",
      "4   118               Tchaikovsky (A)\n",
      "5   143                 Bin Laden (B)\n",
      "6   148                       neither\n",
      "7   156                       neither\n",
      "8   189                    Denton (A)\n",
      "9   190                       neither\n",
      "10  193                     Simone(B)\n",
      "11  228                     Benji (B)\n",
      "12  252   neither (actor v character)\n",
      "13  283                    Torres (B)\n",
      "14  314                 Maldonado (A)\n",
      "15  325                       neither\n",
      "16  372  neither (actor vs character)\n",
      "17  375                           (A)\n",
      "18  380                      Edna (B)\n",
      "19  393                           (B)\n",
      "20  416                      Aiko (A)\n",
      "21  440                           (B)\n",
      "Step LabelSanitizer, transforming completed\n",
      "Step LabelSanitizer, transform completed\n",
      "Step MentionsAnnotator, working in \"val\" mode\n",
      "Step InputReader, working in \"val\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step MentionsAnnotator, unpacking inputs\n",
      "Step MentionsAnnotator, transforming...\n",
      "Applying..: 100%|████████████████████████████| 454/454 [00:00<00:00, 795.07it/s]\n",
      "Step MentionsAnnotator, transforming completed\n",
      "Step MentionsAnnotator, transform completed\n",
      "Step gather_step, adapting inputs\n",
      "Step gather_step, transforming...\n",
      "Step gather_step, transforming completed\n",
      "Step gather_step, transform completed\n",
      "Step gather_step, working in \"test\" mode\n",
      "Step InputReader, working in \"test\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step LabelSanitizer, working in \"test\" mode\n",
      "Step LabelSanitizer, adapting inputs\n",
      "Step LabelSanitizer, transforming...\n",
      "      id                                        label\n",
      "0     28   Juliana (not really ambiguous, but ok) (B)\n",
      "1     31                                   Phelps (B)\n",
      "2     51                                     Carr (A)\n",
      "3     63                             Matthew Hall (B)\n",
      "4     77                           Robert Caspary (B)\n",
      "5     91                           George Clinton (A)\n",
      "6    120                               Chad Brown (B)\n",
      "7    188                                      neither\n",
      "8    205                                   Colvin (A)\n",
      "9    218                                Josephine (A)\n",
      "10   288                           Allison Darren (A)\n",
      "11   357                                 Kristeva (B)\n",
      "12   360                             both are correct\n",
      "13   382                                      neither\n",
      "14   420                       Edward James Olmos (A)\n",
      "15   428                                      neither\n",
      "16   441                                Collinson (A)\n",
      "17   446                                  Granier (A)\n",
      "18   479                                    Carol (B)\n",
      "19   491                                    Harry (B)\n",
      "20   506                                          (B)\n",
      "21   511                                          (B)\n",
      "22   513                                      neither\n",
      "23   526                                      neither\n",
      "24   537                                   Sakshi (B)\n",
      "25   568                             Elise LeGrow (B)\n",
      "26   573                                   Walter (B)\n",
      "27   603                                      neither\n",
      "28   612                                  Georgie (B)\n",
      "29   634                           Paul Ingrassia (B)\n",
      "..   ...                                          ...\n",
      "49  1200                                          (B)\n",
      "50  1236                                   Ingram (B)\n",
      "51  1270                                   Ulrich (B)\n",
      "52  1292                                   Draper (B)\n",
      "53  1299                                Harry Hay (B)\n",
      "54  1334                                    Angie (A)\n",
      "55  1344                                      neither\n",
      "56  1336                                 Zbigniew (A)\n",
      "57  1418                                      neither\n",
      "58  1456                                  Barclay (B)\n",
      "59  1480                                Luke Cage (B)\n",
      "60  1560                                      neither\n",
      "61  1561                                      neither\n",
      "62  1652                                      neither\n",
      "63  1684                                      neither\n",
      "64  1690                                   Severa (B)\n",
      "65  1694                                      neither\n",
      "66  1699                                      neither\n",
      "67  1722                         Marie Antoinette (B)\n",
      "68  1753                                      neither\n",
      "69  1786                                    Oxana (A)\n",
      "70  1805                                  Claudia (B)\n",
      "71  1831                                  Goodson (B)\n",
      "72  1861                             Vivien Leigh (A)\n",
      "73  1885                                      neither\n",
      "74  1903                                     Sosa (B)\n",
      "75  1932                                      neither\n",
      "76  1940                                      neither\n",
      "77  1974                                          (B)\n",
      "78  1997                                      neither\n",
      "\n",
      "[79 rows x 2 columns]\n",
      "Step LabelSanitizer, transforming completed\n",
      "Step LabelSanitizer, transform completed\n",
      "Step MentionsAnnotator, working in \"test\" mode\n",
      "Step InputReader, working in \"test\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step MentionsAnnotator, unpacking inputs\n",
      "Step MentionsAnnotator, transforming...\n",
      "Applying..: 100%|██████████████████████████| 2000/2000 [00:02<00:00, 725.56it/s]\n",
      "Step MentionsAnnotator, transforming completed\n",
      "Step MentionsAnnotator, transform completed\n",
      "Step gather_step, adapting inputs\n",
      "Step gather_step, transforming...\n",
      "Step gather_step, transforming completed\n",
      "Step gather_step, transform completed\n",
      "Step gather_step, working in \"neither\" mode\n",
      "Step InputReader, working in \"neither\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step LabelSanitizer, working in \"neither\" mode\n",
      "Step LabelSanitizer, adapting inputs\n",
      "Step LabelSanitizer, transforming...\n",
      "Empty DataFrame\n",
      "Columns: [id, label]\n",
      "Index: []\n",
      "Step LabelSanitizer, transforming completed\n",
      "Step LabelSanitizer, transform completed\n",
      "Step MentionsAnnotator, working in \"neither\" mode\n",
      "Step InputReader, working in \"neither\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step MentionsAnnotator, unpacking inputs\n",
      "Step MentionsAnnotator, transforming...\n",
      "Applying..: 100%|████████████████████████████| 253/253 [00:00<00:00, 634.78it/s]\n",
      "Step MentionsAnnotator, transforming completed\n",
      "Step MentionsAnnotator, transform completed\n",
      "Step gather_step, adapting inputs\n",
      "Step gather_step, transforming...\n",
      "Step gather_step, transforming completed\n",
      "Step gather_step, transform completed\n",
      "Transforming data to features done.\n",
      " Log a couple of examples for sanity check.\n",
      "\n",
      "Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. Phoebe Thomas played <A> Cheryl Cassidy <A>, <B> Pauline <B>'s friend and also a year 11 pupil in Simon's class. Dumped <P> her <P> boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline.\n",
      "id                                                    development-1\n",
      "text              Zoe Telford -- played the police officer girlf...\n",
      "pronoun                                                         her\n",
      "pronoun_offset                                                  274\n",
      "a                                                    Cheryl Cassidy\n",
      "a_offset                                                        191\n",
      "a_coref                                                        True\n",
      "b                                                           Pauline\n",
      "b_offset                                                        207\n",
      "b_coref                                                       False\n",
      "url               http://en.wikipedia.org/wiki/List_of_Teachers_...\n",
      "label                                                             0\n",
      "pretrained                                       [0.33, 0.33, 0.33]\n",
      "Name: 0, dtype: object\n",
      "Upon their acceptance into the Kontinental Hockey League, Dehner left Finland to sign a contract in Germany with EHC M*nchen of the DEL on June 18, 2014. After capturing the German championship with the M*nchen team in 2016, he left the club and was picked up by fellow DEL side EHC Wolfsburg in July 2016. Former NHLer Gary Suter and Olympic-medalist <A> Bob Suter <A> are <B> Dehner <B>'s uncles. <P> His <P> cousin is Minnesota Wild's alternate captain Ryan Suter.\n",
      "id                                                           test-1\n",
      "text              Upon their acceptance into the Kontinental Hoc...\n",
      "pronoun                                                         His\n",
      "pronoun_offset                                                  383\n",
      "a                                                         Bob Suter\n",
      "a_offset                                                        352\n",
      "a_coref                                                       False\n",
      "b                                                            Dehner\n",
      "b_offset                                                        366\n",
      "b_coref                                                        True\n",
      "url                      http://en.wikipedia.org/wiki/Jeremy_Dehner\n",
      "label                                                             1\n",
      "pretrained                                       [0.33, 0.33, 0.33]\n",
      "Name: 0, dtype: object\n",
      "Indeed, <A> Buster <A> was fully intended to exist in place of Spike for the comic book series, until the release of the Fortress <B> Maximus <B> toy in 1987, which included Spike as a Headmaster partner, hence necessitating the hurried introduction of Spike into the comic book continuity. Returning home from college to discover that <P> his <P> father's garage had been destroyed, Spike investigated the Autobots' deserted base at Mount Saint Hillary, learning that Buster had been captured by the Earth-based Decepticons.\n",
      "id                                                          test-16\n",
      "text              Indeed, <A> Buster <A> was fully intended to e...\n",
      "pronoun                                                         his\n",
      "pronoun_offset                                                  320\n",
      "a                                                            Buster\n",
      "a_offset                                                          8\n",
      "a_coref                                                       False\n",
      "b                                                           Maximus\n",
      "b_offset                                                        122\n",
      "b_coref                                                       False\n",
      "url                     http://en.wikipedia.org/wiki/Spike_Witwicky\n",
      "label                                                             2\n",
      "pretrained                                       [0.33, 0.33, 0.33]\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0604 17:50:43.773911 140448954050304 exec.py:613] Running in train mode. Clearing model output directory.\n",
      "I0604 17:50:43.774212 140448954050304 exec.py:616] Clearing data directory with train, val and test files in bert format.\n",
      "I0604 17:50:43.774656 140448954050304 exec.py:461] device: cuda n_gpu: 3, \n",
      "I0604 17:50:43.942634 140448954050304 tokenization.py:146] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /home/sandeep_attree/.pytorch_pretrained_bert/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "Convert Examples to features: 0it [00:00, ?it/s]I0604 17:50:43.972166 140448954050304 features.py:107] *** Example ***\n",
      "I0604 17:50:43.972329 140448954050304 features.py:108] id: development-1\n",
      "I0604 17:50:43.972393 140448954050304 features.py:109] tokens: [CLS] zoe tel ##ford - - played the police officer girlfriend of simon , maggie . dumped by simon in the final episode of series 1 , after he slept with jenny , and is not seen again . phoebe thomas played < a > cheryl cassidy < a > , < b > pauline < b > ' s friend and also a year 11 pupil in simon ' s class . dumped < p > her < p > boyfriend following simon ' s advice after he wouldn ' t have sex with her but later realised this was due to him catching crabs off her friend pauline . [SEP]\n",
      "I0604 17:50:43.972469 140448954050304 features.py:113] label: 0\n",
      "I0604 17:50:43.972563 140448954050304 features.py:114] pretrained: [0.33, 0.33, 0.33]\n",
      "I0604 17:50:43.972782 140448954050304 features.py:117] GPR tags mask: ['<' 'a' '>' '<' 'a' '>' '<' 'b' '>' '<' 'b' '>' '<' 'p' '>' '<' 'p' '>']\n",
      "I0604 17:50:43.972971 140448954050304 features.py:120] Pronoun tokens: ['her']\n",
      "I0604 17:50:43.973068 140448954050304 features.py:121] A tokens: ['cheryl' 'cassidy']\n",
      "I0604 17:50:43.973147 140448954050304 features.py:122] B tokens: ['pauline']\n",
      "I0604 17:50:43.973275 140448954050304 features.py:127] clusters P model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:43.974426 140448954050304 features.py:131] clusters A model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:43.975580 140448954050304 features.py:135] clusters B model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:43.975708 140448954050304 features.py:127] clusters P model 1: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:43.976921 140448954050304 features.py:131] clusters A model 1: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:43.978080 140448954050304 features.py:135] clusters B model 1: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:43.978206 140448954050304 features.py:127] clusters P model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:43.979346 140448954050304 features.py:131] clusters A model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:43.980488 140448954050304 features.py:135] clusters B model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:43.980610 140448954050304 features.py:127] clusters P model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:43.981764 140448954050304 features.py:131] clusters A model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:43.982910 140448954050304 features.py:135] clusters B model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Convert Examples to features: 2000it [00:03, 509.39it/s]\n",
      "Convert Examples to features: 0it [00:00, ?it/s]I0604 17:50:47.898942 140448954050304 features.py:107] *** Example ***\n",
      "I0604 17:50:47.899206 140448954050304 features.py:108] id: validation-1\n",
      "I0604 17:50:47.899270 140448954050304 features.py:109] tokens: [CLS] he admitted making four trips to china and playing golf there . he also admitted that z ##te officials , whom he says are his golf buddies , hosted and paid for the trips . jose de ve ##ne ##cia iii , son of house speaker < a > jose de ve ##ne ##cia jr < a > , alleged that < b > aba ##los < b > offered < p > him < p > us $ 10 million to withdraw his proposal on the n ##bn project . [SEP]\n",
      "I0604 17:50:47.899334 140448954050304 features.py:113] label: 2\n",
      "I0604 17:50:47.899452 140448954050304 features.py:114] pretrained: [0.33, 0.33, 0.33]\n",
      "I0604 17:50:47.899699 140448954050304 features.py:117] GPR tags mask: ['<' 'a' '>' '<' 'a' '>' '<' 'b' '>' '<' 'b' '>' '<' 'p' '>' '<' 'p' '>']\n",
      "I0604 17:50:47.899867 140448954050304 features.py:120] Pronoun tokens: ['him']\n",
      "I0604 17:50:47.899991 140448954050304 features.py:121] A tokens: ['jose' 'de' 've' '##ne' '##cia' 'jr']\n",
      "I0604 17:50:47.900087 140448954050304 features.py:122] B tokens: ['aba' '##los']\n",
      "I0604 17:50:47.900229 140448954050304 features.py:127] clusters P model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:47.901555 140448954050304 features.py:131] clusters A model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:47.902868 140448954050304 features.py:135] clusters B model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:47.903008 140448954050304 features.py:127] clusters P model 1: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:47.904195 140448954050304 features.py:131] clusters A model 1: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:47.905368 140448954050304 features.py:135] clusters B model 1: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:47.905494 140448954050304 features.py:127] clusters P model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:47.906644 140448954050304 features.py:131] clusters A model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:47.907790 140448954050304 features.py:135] clusters B model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:47.907911 140448954050304 features.py:127] clusters P model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:47.909184 140448954050304 features.py:131] clusters A model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:47.910433 140448954050304 features.py:135] clusters B model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Convert Examples to features: 454it [00:00, 512.22it/s]\n",
      "Convert Examples to features: 0it [00:00, ?it/s]I0604 17:50:48.786081 140448954050304 features.py:107] *** Example ***\n",
      "I0604 17:50:48.786310 140448954050304 features.py:108] id: test-1\n",
      "I0604 17:50:48.786396 140448954050304 features.py:109] tokens: [CLS] upon their acceptance into the ko ##ntine ##ntal hockey league , de ##hner left finland to sign a contract in germany with eh ##c m * nc ##hen of the del on june 18 , 2014 . after capturing the german championship with the m * nc ##hen team in 2016 , he left the club and was picked up by fellow del side eh ##c wolf ##sburg in july 2016 . former nhl ##er gary su ##ter and olympic - medalist < a > bob su ##ter < a > are < b > de ##hner < b > ' s uncles . < p > his < p > cousin is minnesota wild ' s alternate captain ryan su ##ter . [SEP]\n",
      "I0604 17:50:48.786489 140448954050304 features.py:113] label: 1\n",
      "I0604 17:50:48.786581 140448954050304 features.py:114] pretrained: [0.33, 0.33, 0.33]\n",
      "I0604 17:50:48.786839 140448954050304 features.py:117] GPR tags mask: ['<' 'a' '>' '<' 'a' '>' '<' 'b' '>' '<' 'b' '>' '<' 'p' '>' '<' 'p' '>']\n",
      "I0604 17:50:48.787021 140448954050304 features.py:120] Pronoun tokens: ['his']\n",
      "I0604 17:50:48.787126 140448954050304 features.py:121] A tokens: ['bob' 'su' '##ter']\n",
      "I0604 17:50:48.787219 140448954050304 features.py:122] B tokens: ['de' '##hner']\n",
      "I0604 17:50:48.787373 140448954050304 features.py:127] clusters P model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:48.788638 140448954050304 features.py:131] clusters A model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:48.789963 140448954050304 features.py:135] clusters B model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:48.790092 140448954050304 features.py:127] clusters P model 1: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:48.791265 140448954050304 features.py:131] clusters A model 1: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:48.792496 140448954050304 features.py:135] clusters B model 1: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:48.792632 140448954050304 features.py:127] clusters P model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:48.793869 140448954050304 features.py:131] clusters A model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:48.795159 140448954050304 features.py:135] clusters B model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:48.795291 140448954050304 features.py:127] clusters P model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:48.796544 140448954050304 features.py:131] clusters A model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 17:50:48.797807 140448954050304 features.py:135] clusters B model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert Examples to features: 2000it [00:04, 475.82it/s]\n",
      "I0604 17:50:52.986700 140448954050304 exec.py:510] ***** Training *****\n",
      "I0604 17:50:52.986859 140448954050304 exec.py:511]   Num examples = 2000\n",
      "I0604 17:50:52.986942 140448954050304 exec.py:512]   Batch size = 6\n",
      "I0604 17:50:52.986986 140448954050304 exec.py:515] ***** Evaluation *****\n",
      "I0604 17:50:52.987020 140448954050304 exec.py:516]   Num examples = 454\n",
      "I0604 17:50:52.987067 140448954050304 exec.py:517]   Batch size = 32\n",
      "I0604 17:50:52.987105 140448954050304 exec.py:519] ***** Testing *****\n",
      "I0604 17:50:52.987136 140448954050304 exec.py:520]   Num examples = 2000\n",
      "I0604 17:50:52.987176 140448954050304 exec.py:521]   Batch size = 32\n",
      "Preparing Model.\n",
      "I0604 17:50:53.123540 140448954050304 modeling.py:564] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz from cache at /home/sandeep_attree/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05\n",
      "I0604 17:50:53.124635 140448954050304 modeling.py:572] extracting archive file /home/sandeep_attree/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05 to temp dir /tmp/tmpft1o2rzv\n",
      "I0604 17:51:04.410767 140448954050304 modeling.py:579] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0604 17:51:14.978251 140448954050304 modeling.py:629] Weights of ProBERT not initialized from pretrained model: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "I0604 17:51:14.978518 140448954050304 modeling.py:632] Weights from pretrained model not used in ProBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Trn  0, Loss=0.709, Val score=0.431 F1=0.830, Test score=0.427 F1=0.836: 100%|█| 4454/4454 [06:15<00:00,  9.57it/s]\n",
      "Trn  1, Loss=0.254, Val score=0.390 F1=0.877, Test score=0.406 F1=0.884: 100%|█| 4454/4454 [06:04<00:00, 10.90it/s]\n",
      "Trn  2, Loss=0.100, Val score=0.403 F1=0.894, Test score=inf:  55%|▌| 2454/4454 [04:57<03:51,  8.63it/s]  \n",
      "Trn  3, Loss=0.043, Val score=0.486 F1=0.899, Test score=inf:  55%|▌| 2454/4454 [04:55<03:50,  8.67it/s]  \n",
      "Trn  4, Loss=0.024, Val score=0.447 F1=0.899, Test score=inf:  55%|▌| 2454/4454 [04:55<03:55,  8.51it/s]  \n",
      "Fold 0 done in 0:28:26. \n",
      "Test score - 0.4064149153290782\n",
      "Best validation (early stopping) epochs:  1\n",
      "Validation scores:  0.38983809329427793\n",
      "Test scores:  0.4064149153290782\n",
      "Ensembled Test score:  0.4064149153290782\n"
     ]
    }
   ],
   "source": [
    "!python run.py --train --model=probert --language_model=bert-large-uncased --verbose=1 --exp_dir=results/single_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single model performance - GREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "06/04/2019 22:18:12 - INFO - neuralcoref -   Loading model from /home/sandeep_attree/.neuralcoref_cache/neuralcoref\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0604 22:18:12.183774 140124348446464 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "                  id            text pronoun  ...  b_offset b_coref             url\n",
      "0      development-1  Zoe Telford...     her  ...       207   False  http://en.w...\n",
      "1      development-2  He grew up ...     His  ...       251   False  http://en.w...\n",
      "2      development-3  He had been...     his  ...       246    True  http://en.w...\n",
      "3      development-4  The current...     his  ...       336    True  http://en.w...\n",
      "4      development-5  Her Santa F...     She  ...       294    True  http://en.w...\n",
      "...              ...             ...     ...  ...       ...     ...             ...\n",
      "1995  development...  Faye's thir...     her  ...       328    True  http://en.w...\n",
      "1996  development...  The plot of...     her  ...       215    True  http://en.w...\n",
      "1997  development...  Grant playe...     she  ...       266   False  http://en.w...\n",
      "1998  development...  The fashion...     She  ...       208   False  http://en.w...\n",
      "1999  development...  Watkins was...     her  ...       347    True  http://en.w...\n",
      "\n",
      "[2000 rows x 11 columns]\n",
      "                 id            text pronoun  ...  b_offset b_coref             url\n",
      "0      validation-1  He admitted...     him  ...       241   False  http://en.w...\n",
      "1      validation-2  Kathleen No...     She  ...       150    True  http://en.w...\n",
      "2      validation-3  When she re...     his  ...       406    True  http://en.w...\n",
      "3      validation-4  On 19 March...      he  ...       325   False  http://en.w...\n",
      "4      validation-5  By this tim...     she  ...       328    True  http://en.w...\n",
      "..              ...             ...     ...  ...       ...     ...             ...\n",
      "449  validation-450  He then agr...      He  ...       264   False  http://en.w...\n",
      "450  validation-451  Disgusted w...     she  ...       257   False  http://en.w...\n",
      "451  validation-452  She manipul...     she  ...       291    True  http://en.w...\n",
      "452  validation-453  On April 4,...     her  ...       294    True  http://en.w...\n",
      "453  validation-454  Pleasant ex...     him  ...       255   False  http://en.w...\n",
      "\n",
      "[454 rows x 11 columns]\n",
      "             id            text pronoun  ...  b_offset b_coref             url\n",
      "0        test-1  Upon their ...     His  ...       366    True  http://en.w...\n",
      "1        test-2  Between the...     him  ...       390   False  http://en.w...\n",
      "2        test-3  Though his ...      He  ...       295   False  http://en.w...\n",
      "3        test-4  At the tria...     his  ...       536    True  http://en.w...\n",
      "4        test-5  It is about...     his  ...       559   False  http://en.w...\n",
      "...         ...             ...     ...  ...       ...     ...             ...\n",
      "1995  test-1996  The sole ex...     She  ...       432   False  http://en.w...\n",
      "1996  test-1997  According t...     her  ...       404   False  http://en.w...\n",
      "1997  test-1998  In June 200...     She  ...       412   False  http://en.w...\n",
      "1998  test-1999  She was del...     she  ...       274   False  http://en.w...\n",
      "1999  test-2000  Meg and Vic...     her  ...       260   False  http://en.w...\n",
      "\n",
      "[2000 rows x 11 columns]\n",
      "            id            text pronoun  ...  b_offset b_coref             url\n",
      "0      test-16  Indeed, Bus...     his  ...       122   False  http://en.w...\n",
      "1      test-19  On May 17, ...     her  ...       293   False  http://en.w...\n",
      "2      test-24  Jones playe...      He  ...       319   False  http://en.w...\n",
      "3      test-41  However he ...     his  ...       213   False  http://en.w...\n",
      "4      test-46  When Liu Be...      he  ...       136   False  http://en.w...\n",
      "..         ...             ...     ...  ...       ...     ...             ...\n",
      "248  test-1968  Laura was t...     She  ...       189   False  http://en.w...\n",
      "249  test-1975  In the seve...     her  ...       209   False  http://en.w...\n",
      "250  test-1977  She also pl...     She  ...       105   False  http://en.w...\n",
      "251  test-1980  After Richa...     she  ...       392   False  http://en.w...\n",
      "252  test-1993  In a draft ...     her  ...       446   False  http://en.w...\n",
      "\n",
      "[253 rows x 11 columns]\n",
      "Initializing Step InputReader\n",
      "Step InputReader initialized\n",
      "Initializing Step CorefExtractor\n",
      "Step CorefExtractor initialized\n",
      "Initializing Step PretrainedProref\n",
      "Step PretrainedProref initialized\n",
      "Initializing Step LabelSanitizer\n",
      "Step LabelSanitizer initialized\n",
      "Initializing Step CorefAnnotator\n",
      "Step CorefAnnotator initialized\n",
      "Initializing Step MentionsAnnotator\n",
      "Step MentionsAnnotator initialized\n",
      "Initializing Step PretrainedFeatures\n",
      "Step PretrainedFeatures initialized\n",
      "Initializing Step gather_step\n",
      "STEPPY WARNING: Step with name \"CorefExtractor\", already exist. Make sure that all Steps have unique name.\n",
      "Step gather_step initialized\n",
      "Initializing Step InputReader\n",
      "Step InputReader initialized\n",
      "Initializing Step CorefExtractor\n",
      "Step CorefExtractor initialized\n",
      "Initializing Step PretrainedProref\n",
      "Step PretrainedProref initialized\n",
      "Initializing Step LabelSanitizer\n",
      "Step LabelSanitizer initialized\n",
      "Initializing Step CorefAnnotator\n",
      "Step CorefAnnotator initialized\n",
      "Initializing Step MentionsAnnotator\n",
      "Step MentionsAnnotator initialized\n",
      "Initializing Step PretrainedFeatures\n",
      "Step PretrainedFeatures initialized\n",
      "Initializing Step gather_step\n",
      "STEPPY WARNING: Step with name \"CorefExtractor\", already exist. Make sure that all Steps have unique name.\n",
      "Step gather_step initialized\n",
      "Initializing Step InputReader\n",
      "Step InputReader initialized\n",
      "Initializing Step CorefExtractor\n",
      "Step CorefExtractor initialized\n",
      "Initializing Step PretrainedProref\n",
      "Step PretrainedProref initialized\n",
      "Initializing Step LabelSanitizer\n",
      "Step LabelSanitizer initialized\n",
      "Initializing Step CorefAnnotator\n",
      "Step CorefAnnotator initialized\n",
      "Initializing Step MentionsAnnotator\n",
      "Step MentionsAnnotator initialized\n",
      "Initializing Step PretrainedFeatures\n",
      "Step PretrainedFeatures initialized\n",
      "Initializing Step gather_step\n",
      "STEPPY WARNING: Step with name \"CorefExtractor\", already exist. Make sure that all Steps have unique name.\n",
      "Step gather_step initialized\n",
      "Initializing Step InputReader\n",
      "Step InputReader initialized\n",
      "Initializing Step CorefExtractor\n",
      "Step CorefExtractor initialized\n",
      "Initializing Step PretrainedProref\n",
      "Step PretrainedProref initialized\n",
      "Initializing Step LabelSanitizer\n",
      "Step LabelSanitizer initialized\n",
      "Initializing Step CorefAnnotator\n",
      "Step CorefAnnotator initialized\n",
      "Initializing Step MentionsAnnotator\n",
      "Step MentionsAnnotator initialized\n",
      "Initializing Step PretrainedFeatures\n",
      "Step PretrainedFeatures initialized\n",
      "Initializing Step gather_step\n",
      "STEPPY WARNING: Step with name \"CorefExtractor\", already exist. Make sure that all Steps have unique name.\n",
      "Step gather_step initialized\n",
      "{'edges': {('CorefAnnotator', 'MentionsAnnotator'),\n",
      "           ('CorefExtractor', 'CorefAnnotator'),\n",
      "           ('CorefExtractor', 'PretrainedProref'),\n",
      "           ('InputReader', 'gather_step'),\n",
      "           ('LabelSanitizer', 'gather_step'),\n",
      "           ('MentionsAnnotator', 'gather_step'),\n",
      "           ('PretrainedFeatures', 'gather_step'),\n",
      "           ('PretrainedProref', 'PretrainedFeatures'),\n",
      "           ('input', 'CorefAnnotator'),\n",
      "           ('input', 'CorefExtractor'),\n",
      "           ('input', 'InputReader'),\n",
      "           ('input', 'LabelSanitizer'),\n",
      "           ('input', 'PretrainedProref')},\n",
      " 'nodes': {'CorefAnnotator',\n",
      "           'CorefExtractor',\n",
      "           'InputReader',\n",
      "           'LabelSanitizer',\n",
      "           'MentionsAnnotator',\n",
      "           'PretrainedFeatures',\n",
      "           'PretrainedProref',\n",
      "           'gather_step',\n",
      "           'input'}}\n",
      "Transforming data to features.\n",
      "Step gather_step, working in \"train\" mode\n",
      "Step InputReader, working in \"train\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step LabelSanitizer, working in \"train\" mode\n",
      "Step LabelSanitizer, adapting inputs\n",
      "Step LabelSanitizer, transforming...\n",
      "      id                                            label\n",
      "0     14                                       Gracia (B)\n",
      "1     35                                 Lim Goh Tong (B)\n",
      "2     40                                          neither\n",
      "3    104                                Polly Simmons (B)\n",
      "4    120  neither (differentiate between person and role)\n",
      "5    140                                       Isabel (B)\n",
      "6    170                                          Day (B)\n",
      "7    171                                              (A)\n",
      "8    259                                              (B)\n",
      "9    260                                        Smith (A)\n",
      "10   290                                     Lewellyn (A)\n",
      "11   324                                   Umfraville (B)\n",
      "12   327                                          neither\n",
      "13   336                                          Rex (B)\n",
      "14   345                         neither (person vs role)\n",
      "15   347                                          neither\n",
      "16   376                                              (A)\n",
      "17   400                                              (A)\n",
      "18   429                                          neither\n",
      "19   443                                          neither\n",
      "20   456                                    Dan Brown (A)\n",
      "21   467                                          neither\n",
      "22   473                     neither (actor vs character)\n",
      "23   483                                       Huang (B) \n",
      "24   527                                     Cynthia (B) \n",
      "25   540                                Frances Black (B)\n",
      "26   547                                          neither\n",
      "27   565                                       Ragnar (A)\n",
      "28   577                                              (A)\n",
      "29   586                                  Leadbetter (A) \n",
      "..   ...                                              ...\n",
      "43  1050                                          neither\n",
      "44  1085                                          neither\n",
      "45  1160                                              (B)\n",
      "46  1191                                              (A)\n",
      "47  1203                                              (B)\n",
      "48  1215                                              (A)\n",
      "49  1258                                          neither\n",
      "50  1291                                          neither\n",
      "51  1337        (B)  (let’s see what the model does here)\n",
      "52  1368                                              (A)\n",
      "53  1415                                              (B)\n",
      "54  1421                                              (A)\n",
      "55  1454                                              (A)\n",
      "56  1484                                              (B)\n",
      "57  1512                                          neither\n",
      "58  1564                                              (B)\n",
      "59  1568                     neither (actor vs character)\n",
      "60  1569                     neither (actor vs character)\n",
      "61  1572                 neither (or A, definitely not B)\n",
      "62  1614                                              (A)\n",
      "63  1653                                              (B)\n",
      "64  1679                                              (B)\n",
      "65  1709                                              (B)\n",
      "66  1721                     neither (actor vs character)\n",
      "67  1744                                              (B)\n",
      "68  1793                                              (A)\n",
      "69  1804                     neither (actor vs character)\n",
      "70  1873                                              (A)\n",
      "71  1933                                              (A)\n",
      "72  1993                                              (B)\n",
      "\n",
      "[73 rows x 2 columns]\n",
      "Step LabelSanitizer, transforming completed\n",
      "Step LabelSanitizer, transform completed\n",
      "Step MentionsAnnotator, working in \"train\" mode\n",
      "Step CorefAnnotator, working in \"train\" mode\n",
      "Step CorefExtractor, working in \"train\" mode\n",
      "Step CorefExtractor loading persisted output from results/single_model/data_pipeline/output/train/CorefExtractor\n",
      "Step CorefExtractor, loading output from results/single_model/data_pipeline/output/train/CorefExtractor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step CorefExtractor, transform completed\n",
      "Step CorefAnnotator, adapting inputs\n",
      "Step CorefAnnotator, transforming...\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:04<00:00, 433.57it/s]\n",
      "Step CorefAnnotator, transforming completed\n",
      "Step CorefAnnotator, transform completed\n",
      "Step MentionsAnnotator, unpacking inputs\n",
      "Step MentionsAnnotator, transforming...\n",
      "Applying..: 100%|██████████████████████████| 2000/2000 [00:02<00:00, 677.86it/s]\n",
      "Step MentionsAnnotator, transforming completed\n",
      "Step MentionsAnnotator, transform completed\n",
      "Step PretrainedFeatures, working in \"train\" mode\n",
      "Step PretrainedProref, working in \"train\" mode\n",
      "Step PretrainedProref loading persisted output from results/single_model/data_pipeline/output/train/PretrainedProref\n",
      "Step PretrainedProref, loading output from results/single_model/data_pipeline/output/train/PretrainedProref\n",
      "Step PretrainedProref, transform completed\n",
      "Step PretrainedFeatures, unpacking inputs\n",
      "Step PretrainedFeatures, transforming...\n",
      "Step PretrainedFeatures, transforming completed\n",
      "Step PretrainedFeatures, transform completed\n",
      "Step gather_step, adapting inputs\n",
      "Step gather_step, transforming...\n",
      "Step gather_step, transforming completed\n",
      "Step gather_step, transform completed\n",
      "Step gather_step, working in \"val\" mode\n",
      "Step InputReader, working in \"val\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step LabelSanitizer, working in \"val\" mode\n",
      "Step LabelSanitizer, adapting inputs\n",
      "Step LabelSanitizer, transforming...\n",
      "     id                         label\n",
      "0    29                        either\n",
      "1    79                   Lillian (B)\n",
      "2    80              Edward Jones (A)\n",
      "3   111               Jane Austen (A)\n",
      "4   118               Tchaikovsky (A)\n",
      "5   143                 Bin Laden (B)\n",
      "6   148                       neither\n",
      "7   156                       neither\n",
      "8   189                    Denton (A)\n",
      "9   190                       neither\n",
      "10  193                     Simone(B)\n",
      "11  228                     Benji (B)\n",
      "12  252   neither (actor v character)\n",
      "13  283                    Torres (B)\n",
      "14  314                 Maldonado (A)\n",
      "15  325                       neither\n",
      "16  372  neither (actor vs character)\n",
      "17  375                           (A)\n",
      "18  380                      Edna (B)\n",
      "19  393                           (B)\n",
      "20  416                      Aiko (A)\n",
      "21  440                           (B)\n",
      "Step LabelSanitizer, transforming completed\n",
      "Step LabelSanitizer, transform completed\n",
      "Step MentionsAnnotator, working in \"val\" mode\n",
      "Step CorefAnnotator, working in \"val\" mode\n",
      "Step CorefExtractor, working in \"val\" mode\n",
      "Step CorefExtractor loading persisted output from results/single_model/data_pipeline/output/val/CorefExtractor\n",
      "Step CorefExtractor, loading output from results/single_model/data_pipeline/output/val/CorefExtractor\n",
      "Step CorefExtractor, transform completed\n",
      "Step CorefAnnotator, adapting inputs\n",
      "Step CorefAnnotator, transforming...\n",
      "100%|████████████████████████████████████████| 454/454 [00:00<00:00, 499.94it/s]\n",
      "Step CorefAnnotator, transforming completed\n",
      "Step CorefAnnotator, transform completed\n",
      "Step MentionsAnnotator, unpacking inputs\n",
      "Step MentionsAnnotator, transforming...\n",
      "Applying..: 100%|████████████████████████████| 454/454 [00:00<00:00, 700.51it/s]\n",
      "Step MentionsAnnotator, transforming completed\n",
      "Step MentionsAnnotator, transform completed\n",
      "Step PretrainedFeatures, working in \"val\" mode\n",
      "Step PretrainedProref, working in \"val\" mode\n",
      "Step PretrainedProref loading persisted output from results/single_model/data_pipeline/output/val/PretrainedProref\n",
      "Step PretrainedProref, loading output from results/single_model/data_pipeline/output/val/PretrainedProref\n",
      "Step PretrainedProref, transform completed\n",
      "Step PretrainedFeatures, unpacking inputs\n",
      "Step PretrainedFeatures, transforming...\n",
      "Step PretrainedFeatures, transforming completed\n",
      "Step PretrainedFeatures, transform completed\n",
      "Step gather_step, adapting inputs\n",
      "Step gather_step, transforming...\n",
      "Step gather_step, transforming completed\n",
      "Step gather_step, transform completed\n",
      "Step gather_step, working in \"test\" mode\n",
      "Step InputReader, working in \"test\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step LabelSanitizer, working in \"test\" mode\n",
      "Step LabelSanitizer, adapting inputs\n",
      "Step LabelSanitizer, transforming...\n",
      "      id                                        label\n",
      "0     28   Juliana (not really ambiguous, but ok) (B)\n",
      "1     31                                   Phelps (B)\n",
      "2     51                                     Carr (A)\n",
      "3     63                             Matthew Hall (B)\n",
      "4     77                           Robert Caspary (B)\n",
      "5     91                           George Clinton (A)\n",
      "6    120                               Chad Brown (B)\n",
      "7    188                                      neither\n",
      "8    205                                   Colvin (A)\n",
      "9    218                                Josephine (A)\n",
      "10   288                           Allison Darren (A)\n",
      "11   357                                 Kristeva (B)\n",
      "12   360                             both are correct\n",
      "13   382                                      neither\n",
      "14   420                       Edward James Olmos (A)\n",
      "15   428                                      neither\n",
      "16   441                                Collinson (A)\n",
      "17   446                                  Granier (A)\n",
      "18   479                                    Carol (B)\n",
      "19   491                                    Harry (B)\n",
      "20   506                                          (B)\n",
      "21   511                                          (B)\n",
      "22   513                                      neither\n",
      "23   526                                      neither\n",
      "24   537                                   Sakshi (B)\n",
      "25   568                             Elise LeGrow (B)\n",
      "26   573                                   Walter (B)\n",
      "27   603                                      neither\n",
      "28   612                                  Georgie (B)\n",
      "29   634                           Paul Ingrassia (B)\n",
      "..   ...                                          ...\n",
      "49  1200                                          (B)\n",
      "50  1236                                   Ingram (B)\n",
      "51  1270                                   Ulrich (B)\n",
      "52  1292                                   Draper (B)\n",
      "53  1299                                Harry Hay (B)\n",
      "54  1334                                    Angie (A)\n",
      "55  1344                                      neither\n",
      "56  1336                                 Zbigniew (A)\n",
      "57  1418                                      neither\n",
      "58  1456                                  Barclay (B)\n",
      "59  1480                                Luke Cage (B)\n",
      "60  1560                                      neither\n",
      "61  1561                                      neither\n",
      "62  1652                                      neither\n",
      "63  1684                                      neither\n",
      "64  1690                                   Severa (B)\n",
      "65  1694                                      neither\n",
      "66  1699                                      neither\n",
      "67  1722                         Marie Antoinette (B)\n",
      "68  1753                                      neither\n",
      "69  1786                                    Oxana (A)\n",
      "70  1805                                  Claudia (B)\n",
      "71  1831                                  Goodson (B)\n",
      "72  1861                             Vivien Leigh (A)\n",
      "73  1885                                      neither\n",
      "74  1903                                     Sosa (B)\n",
      "75  1932                                      neither\n",
      "76  1940                                      neither\n",
      "77  1974                                          (B)\n",
      "78  1997                                      neither\n",
      "\n",
      "[79 rows x 2 columns]\n",
      "Step LabelSanitizer, transforming completed\n",
      "Step LabelSanitizer, transform completed\n",
      "Step MentionsAnnotator, working in \"test\" mode\n",
      "Step CorefAnnotator, working in \"test\" mode\n",
      "Step CorefExtractor, working in \"test\" mode\n",
      "Step CorefExtractor loading persisted output from results/single_model/data_pipeline/output/test/CorefExtractor\n",
      "Step CorefExtractor, loading output from results/single_model/data_pipeline/output/test/CorefExtractor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step CorefExtractor, transform completed\n",
      "Step CorefAnnotator, adapting inputs\n",
      "Step CorefAnnotator, transforming...\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:04<00:00, 413.06it/s]\n",
      "Step CorefAnnotator, transforming completed\n",
      "Step CorefAnnotator, transform completed\n",
      "Step MentionsAnnotator, unpacking inputs\n",
      "Step MentionsAnnotator, transforming...\n",
      "Applying..: 100%|██████████████████████████| 2000/2000 [00:02<00:00, 684.68it/s]\n",
      "Step MentionsAnnotator, transforming completed\n",
      "Step MentionsAnnotator, transform completed\n",
      "Step PretrainedFeatures, working in \"test\" mode\n",
      "Step PretrainedProref, working in \"test\" mode\n",
      "Step PretrainedProref loading persisted output from results/single_model/data_pipeline/output/test/PretrainedProref\n",
      "Step PretrainedProref, loading output from results/single_model/data_pipeline/output/test/PretrainedProref\n",
      "Step PretrainedProref, transform completed\n",
      "Step PretrainedFeatures, unpacking inputs\n",
      "Step PretrainedFeatures, transforming...\n",
      "Step PretrainedFeatures, transforming completed\n",
      "Step PretrainedFeatures, transform completed\n",
      "Step gather_step, adapting inputs\n",
      "Step gather_step, transforming...\n",
      "Step gather_step, transforming completed\n",
      "Step gather_step, transform completed\n",
      "Step gather_step, working in \"neither\" mode\n",
      "Step InputReader, working in \"neither\" mode\n",
      "Step InputReader, adapting inputs\n",
      "Step InputReader, transforming...\n",
      "Step InputReader, transforming completed\n",
      "Step InputReader, transform completed\n",
      "Step LabelSanitizer, working in \"neither\" mode\n",
      "Step LabelSanitizer, adapting inputs\n",
      "Step LabelSanitizer, transforming...\n",
      "Empty DataFrame\n",
      "Columns: [id, label]\n",
      "Index: []\n",
      "Step LabelSanitizer, transforming completed\n",
      "Step LabelSanitizer, transform completed\n",
      "Step MentionsAnnotator, working in \"neither\" mode\n",
      "Step CorefAnnotator, working in \"neither\" mode\n",
      "Step CorefExtractor, working in \"neither\" mode\n",
      "Step CorefExtractor loading persisted output from results/single_model/data_pipeline/output/neither/CorefExtractor\n",
      "Step CorefExtractor, loading output from results/single_model/data_pipeline/output/neither/CorefExtractor\n",
      "Step CorefExtractor, transform completed\n",
      "Step CorefAnnotator, adapting inputs\n",
      "Step CorefAnnotator, transforming...\n",
      "100%|████████████████████████████████████████| 253/253 [00:00<00:00, 524.71it/s]\n",
      "Step CorefAnnotator, transforming completed\n",
      "Step CorefAnnotator, transform completed\n",
      "Step MentionsAnnotator, unpacking inputs\n",
      "Step MentionsAnnotator, transforming...\n",
      "Applying..: 100%|████████████████████████████| 253/253 [00:00<00:00, 704.76it/s]\n",
      "Step MentionsAnnotator, transforming completed\n",
      "Step MentionsAnnotator, transform completed\n",
      "Step PretrainedFeatures, working in \"neither\" mode\n",
      "Step PretrainedProref, working in \"neither\" mode\n",
      "Step PretrainedProref loading persisted output from results/single_model/data_pipeline/output/neither/PretrainedProref\n",
      "Step PretrainedProref, loading output from results/single_model/data_pipeline/output/neither/PretrainedProref\n",
      "Step PretrainedProref, transform completed\n",
      "Step PretrainedFeatures, unpacking inputs\n",
      "Step PretrainedFeatures, transforming...\n",
      "Step PretrainedFeatures, transforming completed\n",
      "Step PretrainedFeatures, transform completed\n",
      "Step gather_step, adapting inputs\n",
      "Step gather_step, transforming...\n",
      "Step gather_step, transforming completed\n",
      "Step gather_step, transform completed\n",
      "Transforming data to features done.\n",
      " Log a couple of examples for sanity check.\n",
      "\n",
      "Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. <E_1><E_2><E_3>Phoebe Thomas<E_1><E_2><E_3> played <C_1><D_1><A> Cheryl Cassidy <A>, <D_3><B> Pauline <B>'s<D_3> friend<C_1><D_1> and also a year 11 pupil in Simon's class. Dumped <E_1><E_2><E_3><P> her <P><E_1><E_2><E_3> boyfriend following Simon's advice after he wouldn't have sex with <E_1><E_2><E_3>her<E_1><E_2><E_3> but later realised this was due to him catching crabs off <C_1><D_1><E_1><E_2><D_3><E_3>her<E_1><E_2><E_3> friend Pauline<C_1><D_1><D_3>.\n",
      "id                                                    development-1\n",
      "text              Zoe Telford -- played the police officer girlf...\n",
      "pronoun                                                         her\n",
      "pronoun_offset                                                  349\n",
      "a                                                    Cheryl Cassidy\n",
      "a_offset                                                        231\n",
      "a_coref                                                        True\n",
      "b                                                           Pauline\n",
      "b_offset                                                        252\n",
      "b_coref                                                       False\n",
      "url               http://en.wikipedia.org/wiki/List_of_Teachers_...\n",
      "label                                                             0\n",
      "pretrained        [False, False, False, False, False, False, Fal...\n",
      "Name: 0, dtype: object\n",
      "Upon <D_3><E_3>their<D_3><E_3> acceptance into the Kontinental Hockey League, <D_1><D_3><E_3>Dehner<D_1><D_3><E_3> left Finland to sign a contract in Germany with <C_1>EHC M*nchen of <D_1>the DEL<C_1><D_1> on June 18, 2014. After capturing the German championship with the <D_1>M*nchen<D_1> team in 2016, <E_1><E_2><D_3><E_3>he<E_1><E_2><D_3><E_3> left the club and was picked up by fellow DEL side EHC Wolfsburg in July 2016. Former NHLer Gary Suter and <C_1>Olympic-medalist <A> Bob Suter <A><C_1> are <D_0><E_0><D_1><D_3><E_3><B> Dehner <B><D_0><E_0>'s<D_1><D_3><E_3> uncles. <D_0><E_0><E_1><E_2><D_3><E_3><P> His <P><D_0><E_0><E_1><E_2><D_3><E_3> cousin is Minnesota Wild's alternate captain Ryan Suter.\n",
      "id                                                           test-1\n",
      "text              Upon <D_3><E_3>their<D_3><E_3> acceptance into...\n",
      "pronoun                                                         His\n",
      "pronoun_offset                                                  593\n",
      "a                                                         Bob Suter\n",
      "a_offset                                                        477\n",
      "a_coref                                                       False\n",
      "b                                                            Dehner\n",
      "b_offset                                                        521\n",
      "b_coref                                                        True\n",
      "url                      http://en.wikipedia.org/wiki/Jeremy_Dehner\n",
      "label                                                             1\n",
      "pretrained        [False, False, False, False, True, False, Fals...\n",
      "Name: 0, dtype: object\n",
      "Indeed, <C_1><E_1><C_2><C_3><E_3><A> Buster <A><C_1><E_1><C_2><C_3><E_3> was fully intended to exist in place of <C_1><E_1>Spike<C_1><E_1> for the comic book series, until the release of the Fortress <B> Maximus <B> toy in 1987, which included <C_1><E_1>Spike<C_1><E_1> as a <C_1><E_1>Headmaster<C_1><E_1> partner, hence <C_1><E_1>necessitating<C_1><E_1> the hurried introduction of <C_1><E_1>Spike<C_1><E_1> into the comic book continuity. Returning home from college to discover that <C_1><E_1><C_3><E_3><P> his <P><C_1><E_1><C_3><E_3> father's garage had been destroyed, <C_1><E_1>Spike<C_1><E_1> investigated <C_1><E_1>the Autobots'<C_1><E_1> deserted base at Mount Saint Hillary, learning that <C_1><E_1><C_2><C_3><E_3>Buster<C_1><E_1><C_2><C_3><E_3> had been captured by <C_1><E_1>the Earth-based Decepticons<C_1><E_1>.\n",
      "id                                                          test-16\n",
      "text              Indeed, <C_1><E_1><C_2><C_3><E_3><A> Buster <A...\n",
      "pronoun                                                         his\n",
      "pronoun_offset                                                  490\n",
      "a                                                            Buster\n",
      "a_offset                                                         33\n",
      "a_coref                                                       False\n",
      "b                                                           Maximus\n",
      "b_offset                                                        192\n",
      "b_coref                                                       False\n",
      "url                     http://en.wikipedia.org/wiki/Spike_Witwicky\n",
      "label                                                             2\n",
      "pretrained        [False, True, False, True, False, False, False...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0604 22:18:36.749231 140124348446464 exec.py:613] Running in train mode. Clearing model output directory.\n",
      "I0604 22:18:36.882823 140124348446464 exec.py:616] Clearing data directory with train, val and test files in bert format.\n",
      "I0604 22:18:36.883278 140124348446464 exec.py:461] device: cuda n_gpu: 3, \n",
      "I0604 22:18:37.036731 140124348446464 tokenization.py:146] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /home/sandeep_attree/.pytorch_pretrained_bert/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "Convert Examples to features: 0it [00:00, ?it/s]I0604 22:18:37.069851 140124348446464 features.py:107] *** Example ***\n",
      "I0604 22:18:37.070039 140124348446464 features.py:108] id: development-1\n",
      "I0604 22:18:37.070119 140124348446464 features.py:109] tokens: [CLS] zoe tel ##ford - - played the police officer girlfriend of simon , maggie . dumped by simon in the final episode of series 1 , after he slept with jenny , and is not seen again . phoebe thomas played < a > cheryl cassidy < a > , < b > pauline < b > ' s friend and also a year 11 pupil in simon ' s class . dumped < p > her < p > boyfriend following simon ' s advice after he wouldn ' t have sex with her but later realised this was due to him catching crabs off her friend pauline . [SEP]\n",
      "I0604 22:18:37.070191 140124348446464 features.py:113] label: 0\n",
      "I0604 22:18:37.070286 140124348446464 features.py:114] pretrained: [False, False, False, False, False, False, False, False, True, True, True, True]\n",
      "I0604 22:18:37.070533 140124348446464 features.py:117] GPR tags mask: ['<' 'a' '>' '<' 'a' '>' '<' 'b' '>' '<' 'b' '>' '<' 'p' '>' '<' 'p' '>']\n",
      "I0604 22:18:37.070739 140124348446464 features.py:120] Pronoun tokens: ['her']\n",
      "I0604 22:18:37.070856 140124348446464 features.py:121] A tokens: ['cheryl' 'cassidy']\n",
      "I0604 22:18:37.070951 140124348446464 features.py:122] B tokens: ['pauline']\n",
      "I0604 22:18:37.071111 140124348446464 features.py:127] clusters P model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:37.072513 140124348446464 features.py:131] clusters A model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:37.073899 140124348446464 features.py:135] clusters B model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:37.074046 140124348446464 features.py:127] clusters P model 1: [['phoebe', 'thomas'], ['her'], ['her'], ['her'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:37.075399 140124348446464 features.py:131] clusters A model 1: [['cheryl', 'cassidy', ',', 'pauline', \"'\", 's', 'friend'], ['her', 'friend', 'pauline'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:37.076640 140124348446464 features.py:135] clusters B model 1: [['cheryl', 'cassidy', ',', 'pauline', \"'\", 's', 'friend'], ['her', 'friend', 'pauline'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:37.076773 140124348446464 features.py:127] clusters P model 2: [['phoebe', 'thomas'], ['her'], ['her'], ['her'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:37.078061 140124348446464 features.py:131] clusters A model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:37.079310 140124348446464 features.py:135] clusters B model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:37.079446 140124348446464 features.py:127] clusters P model 3: [['phoebe', 'thomas'], ['her'], ['her'], ['her'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:37.080779 140124348446464 features.py:131] clusters A model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:37.082090 140124348446464 features.py:135] clusters B model 3: [['pauline', \"'\", 's'], ['her', 'friend', 'pauline'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Convert Examples to features: 2000it [00:08, 237.70it/s]\n",
      "Convert Examples to features: 0it [00:00, ?it/s]I0604 22:18:45.484355 140124348446464 features.py:107] *** Example ***\n",
      "I0604 22:18:45.484566 140124348446464 features.py:108] id: validation-1\n",
      "I0604 22:18:45.484645 140124348446464 features.py:109] tokens: [CLS] he admitted making four trips to china and playing golf there . he also admitted that z ##te officials , whom he says are his golf buddies , hosted and paid for the trips . jose de ve ##ne ##cia iii , son of house speaker < a > jose de ve ##ne ##cia jr < a > , alleged that < b > aba ##los < b > offered < p > him < p > us $ 10 million to withdraw his proposal on the n ##bn project . [SEP]\n",
      "I0604 22:18:45.484725 140124348446464 features.py:113] label: 2\n",
      "I0604 22:18:45.484808 140124348446464 features.py:114] pretrained: [False, False, True, True, False, False, True, False, True, True, False, False]\n",
      "I0604 22:18:45.485024 140124348446464 features.py:117] GPR tags mask: ['<' 'a' '>' '<' 'a' '>' '<' 'b' '>' '<' 'b' '>' '<' 'p' '>' '<' 'p' '>']\n",
      "I0604 22:18:45.485181 140124348446464 features.py:120] Pronoun tokens: ['him']\n",
      "I0604 22:18:45.485284 140124348446464 features.py:121] A tokens: ['jose' 'de' 've' '##ne' '##cia' 'jr']\n",
      "I0604 22:18:45.485368 140124348446464 features.py:122] B tokens: ['aba' '##los']\n",
      "I0604 22:18:45.485497 140124348446464 features.py:127] clusters P model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:45.486668 140124348446464 features.py:131] clusters A model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:45.487867 140124348446464 features.py:135] clusters B model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:45.488011 140124348446464 features.py:127] clusters P model 1: [['he'], ['he'], ['he'], ['his'], ['him'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:45.489219 140124348446464 features.py:131] clusters A model 1: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:45.490427 140124348446464 features.py:135] clusters B model 1: [['z', '##te'], ['aba', '##los'], ['n', '##bn'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:45.490565 140124348446464 features.py:127] clusters P model 2: [['house', 'speaker', 'jose', 'de', 've', '##ne', '##cia', 'jr'], ['aba', '##los'], ['him'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:45.491729 140124348446464 features.py:131] clusters A model 2: [['house', 'speaker', 'jose', 'de', 've', '##ne', '##cia', 'jr'], ['aba', '##los'], ['him'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:45.492919 140124348446464 features.py:135] clusters B model 2: [['house', 'speaker', 'jose', 'de', 've', '##ne', '##cia', 'jr'], ['aba', '##los'], ['him'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:45.493051 140124348446464 features.py:127] clusters P model 3: [['him'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:45.494205 140124348446464 features.py:131] clusters A model 3: [['him'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:45.495388 140124348446464 features.py:135] clusters B model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Convert Examples to features: 454it [00:01, 263.95it/s]\n",
      "Convert Examples to features: 0it [00:00, ?it/s]I0604 22:18:47.243879 140124348446464 features.py:107] *** Example ***\n",
      "I0604 22:18:47.244068 140124348446464 features.py:108] id: test-1\n",
      "I0604 22:18:47.244149 140124348446464 features.py:109] tokens: [CLS] upon their acceptance into the ko ##ntine ##ntal hockey league , de ##hner left finland to sign a contract in germany with eh ##c m * nc ##hen of the del on june 18 , 2014 . after capturing the german championship with the m * nc ##hen team in 2016 , he left the club and was picked up by fellow del side eh ##c wolf ##sburg in july 2016 . former nhl ##er gary su ##ter and olympic - medalist < a > bob su ##ter < a > are < b > de ##hner < b > ' s uncles . < p > his < p > cousin is minnesota wild ' s alternate captain ryan su ##ter . [SEP]\n",
      "I0604 22:18:47.244222 140124348446464 features.py:113] label: 1\n",
      "I0604 22:18:47.244303 140124348446464 features.py:114] pretrained: [False, False, False, False, True, False, False, True, False, True, True, False]\n",
      "I0604 22:18:47.244496 140124348446464 features.py:117] GPR tags mask: ['<' 'a' '>' '<' 'a' '>' '<' 'b' '>' '<' 'b' '>' '<' 'p' '>' '<' 'p' '>']\n",
      "I0604 22:18:47.244671 140124348446464 features.py:120] Pronoun tokens: ['his']\n",
      "I0604 22:18:47.244769 140124348446464 features.py:121] A tokens: ['bob' 'su' '##ter']\n",
      "I0604 22:18:47.244877 140124348446464 features.py:122] B tokens: ['de' '##hner']\n",
      "I0604 22:18:47.245012 140124348446464 features.py:127] clusters P model 0: [['de', '##hner'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:47.246175 140124348446464 features.py:131] clusters A model 0: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:47.247339 140124348446464 features.py:135] clusters B model 0: [['de', '##hner'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:47.247468 140124348446464 features.py:127] clusters P model 1: [['he'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:47.248623 140124348446464 features.py:131] clusters A model 1: [['olympic', '-', 'medalist', 'bob', 'su', '##ter'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:47.249806 140124348446464 features.py:135] clusters B model 1: [['de', '##hner'], ['the', 'del'], ['m', '*', 'nc', '##hen'], ['de', '##hner', \"'\", 's'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:47.249940 140124348446464 features.py:127] clusters P model 2: [['he'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:47.251153 140124348446464 features.py:131] clusters A model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:47.252340 140124348446464 features.py:135] clusters B model 2: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:47.252477 140124348446464 features.py:127] clusters P model 3: [['their'], ['de', '##hner'], ['he'], ['de', '##hner', \"'\", 's'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:47.253649 140124348446464 features.py:131] clusters A model 3: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "I0604 22:18:47.254813 140124348446464 features.py:135] clusters B model 3: [['their'], ['de', '##hner'], ['he'], ['de', '##hner', \"'\", 's'], ['his'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert Examples to features: 2000it [00:08, 240.09it/s]\n",
      "I0604 22:18:55.570350 140124348446464 exec.py:510] ***** Training *****\n",
      "I0604 22:18:55.570565 140124348446464 exec.py:511]   Num examples = 2000\n",
      "I0604 22:18:55.570662 140124348446464 exec.py:512]   Batch size = 6\n",
      "I0604 22:18:55.570724 140124348446464 exec.py:515] ***** Evaluation *****\n",
      "I0604 22:18:55.570767 140124348446464 exec.py:516]   Num examples = 454\n",
      "I0604 22:18:55.570813 140124348446464 exec.py:517]   Batch size = 32\n",
      "I0604 22:18:55.570851 140124348446464 exec.py:519] ***** Testing *****\n",
      "I0604 22:18:55.570888 140124348446464 exec.py:520]   Num examples = 2000\n",
      "I0604 22:18:55.570977 140124348446464 exec.py:521]   Batch size = 32\n",
      "Preparing Model.\n",
      "I0604 22:18:55.718720 140124348446464 modeling.py:564] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz from cache at /home/sandeep_attree/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05\n",
      "I0604 22:18:55.719645 140124348446464 modeling.py:572] extracting archive file /home/sandeep_attree/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05 to temp dir /tmp/tmpeo60ob01\n",
      "I0604 22:19:07.091615 140124348446464 modeling.py:579] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0604 22:19:17.773855 140124348446464 modeling.py:629] Weights of GREP not initialized from pretrained model: ['pooler.dense.weight', 'pooler.dense.bias', 'evidence_pooler_p.selfattn_mention.FFN.0.weight', 'evidence_pooler_p.selfattn_mention.FFN.0.bias', 'evidence_pooler_p.selfattn_mention.FFN.2.weight', 'evidence_pooler_p.selfattn_mention.FFN.2.bias', 'evidence_pooler_p.selfattn_mention.FFN2.0.weight', 'evidence_pooler_p.selfattn_mention.FFN2.0.bias', 'evidence_pooler_p.selfattn_mention._self_attentive_pooling_projection.0.weight', 'evidence_pooler_p.selfattn_mention._self_attentive_pooling_projection.0.bias', 'evidence_pooler_p.selfattn_mention._self_attentive_pooling_projection.2.weight', 'evidence_pooler_p.selfattn_mention._self_attentive_pooling_projection.2.bias', 'evidence_pooler_p.selfattn_mention._output_layer.0.weight', 'evidence_pooler_p.selfattn_mention._output_layer.0.bias', 'evidence_pooler_p.selfattn_pronoun.FFN.0.weight', 'evidence_pooler_p.selfattn_pronoun.FFN.0.bias', 'evidence_pooler_p.selfattn_pronoun.FFN.2.weight', 'evidence_pooler_p.selfattn_pronoun.FFN.2.bias', 'evidence_pooler_p.selfattn_pronoun.FFN2.0.weight', 'evidence_pooler_p.selfattn_pronoun.FFN2.0.bias', 'evidence_pooler_p.selfattn_pronoun._self_attentive_pooling_projection.0.weight', 'evidence_pooler_p.selfattn_pronoun._self_attentive_pooling_projection.0.bias', 'evidence_pooler_p.selfattn_pronoun._self_attentive_pooling_projection.2.weight', 'evidence_pooler_p.selfattn_pronoun._self_attentive_pooling_projection.2.bias', 'evidence_pooler_p.selfattn_pronoun._output_layer.0.weight', 'evidence_pooler_p.selfattn_pronoun._output_layer.0.bias', 'evidence_pooler_p.selfattn_coref_models.FFN.0.weight', 'evidence_pooler_p.selfattn_coref_models.FFN.0.bias', 'evidence_pooler_p.selfattn_coref_models.FFN.2.weight', 'evidence_pooler_p.selfattn_coref_models.FFN.2.bias', 'evidence_pooler_p.selfattn_coref_models.FFN2.0.weight', 'evidence_pooler_p.selfattn_coref_models.FFN2.0.bias', 'evidence_pooler_p.selfattn_coref_models._self_attentive_pooling_projection.0.weight', 'evidence_pooler_p.selfattn_coref_models._self_attentive_pooling_projection.0.bias', 'evidence_pooler_p.selfattn_coref_models._self_attentive_pooling_projection.2.weight', 'evidence_pooler_p.selfattn_coref_models._self_attentive_pooling_projection.2.bias', 'evidence_pooler_p.selfattn_coref_models._output_layer.0.weight', 'evidence_pooler_p.selfattn_coref_models._output_layer.0.bias', 'evidence_pooler_p.coattn_pronoun.self.query.weight', 'evidence_pooler_p.coattn_pronoun.self.query.bias', 'evidence_pooler_p.coattn_pronoun.self.key.weight', 'evidence_pooler_p.coattn_pronoun.self.key.bias', 'evidence_pooler_p.coattn_pronoun.self.value.weight', 'evidence_pooler_p.coattn_pronoun.self.value.bias', 'evidence_pooler_p.coattn_pronoun.output.dense.weight', 'evidence_pooler_p.coattn_pronoun.output.dense.bias', 'evidence_pooler_p.coattn_pronoun.output.LayerNorm.weight', 'evidence_pooler_p.coattn_pronoun.output.LayerNorm.bias', 'evidence_pooler_p.y_fine.0.weight', 'evidence_pooler_p.y_fine.0.bias', 'evidence_pooler_p.compatability.0.weight', 'evidence_pooler_p.compatability.0.bias', 'classifier.weight', 'classifier.bias']\n",
      "I0604 22:19:17.774080 140124348446464 modeling.py:632] Weights from pretrained model not used in GREP: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Trn  0, Loss=0.583, Val score=0.382 F1=0.861, Test score=0.362 F1=0.893: 100%|█| 4454/4454 [06:32<00:00,  9.50it/s]\n",
      "Trn  1, Loss=0.183, Val score=0.371 F1=0.892, Test score=0.322 F1=0.915: 100%|█| 4454/4454 [06:21<00:00, 10.56it/s]\n",
      "Trn  2, Loss=0.056, Val score=0.414 F1=0.905, Test score=inf:  55%|▌| 2454/4454 [05:14<04:06,  8.12it/s]  \n",
      "Trn  3, Loss=0.024, Val score=0.443 F1=0.910, Test score=inf:  55%|▌| 2454/4454 [05:15<04:00,  8.30it/s]  \n",
      "Trn  4, Loss=0.015, Val score=0.579 F1=0.896, Test score=inf:  55%|▌| 2454/4454 [05:12<04:03,  8.20it/s]  \n",
      "Fold 0 done in 0:30:01. \n",
      "Test score - 0.32202769964930483\n",
      "Best validation (early stopping) epochs:  1\n",
      "Validation scores:  0.37119006117552494\n",
      "Test scores:  0.32202769964930483\n",
      "Ensembled Test score:  0.32202769964930483\n"
     ]
    }
   ],
   "source": [
    "!python run.py --train --model=grep --language_model=bert-large-uncased --verbose=1 --exp_dir=results/single_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle submission model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "06/04/2019 23:17:07 - INFO - neuralcoref -   Loading model from /home/sandeep_attree/.neuralcoref_cache/neuralcoref\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0604 23:17:07.584630 140623027300096 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "I0604 23:17:18.081102 140623027300096 embedding.py:189] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0604 23:17:18.081413 140623027300096 embedding.py:189] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "I0604 23:17:18.081493 140623027300096 embedding.py:189] Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "/usr/local/lib/python3.6/dist-packages/allennlp/data/token_indexers/token_characters_indexer.py:51: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n",
      "W0604 23:17:24.058455 140623027300096 initializers.py:308] Did not use initialization regex that was passed: .*weight_hh.*\n",
      "W0604 23:17:24.058657 140623027300096 initializers.py:308] Did not use initialization regex that was passed: .*weight_ih.*\n",
      "W0604 23:17:24.058709 140623027300096 initializers.py:308] Did not use initialization regex that was passed: .*bias_hh.*\n",
      "W0604 23:17:24.058753 140623027300096 initializers.py:308] Did not use initialization regex that was passed: .*bias_ih.*\n",
      "Setting CUDA_VISIBLE_DEVICES to: 0\n",
      "Loading word embeddings from externals/data/glove.840B.300d.txt...\n",
      "Done loading word embeddings.\n",
      "Loading word embeddings from externals/data/glove_50_300_2.txt...\n",
      "Done loading word embeddings.\n",
      "I0604 23:20:19.029508 140623027300096 resolver.py:79] Using /tmp/tfhub_modules to cache modules.\n",
      "2019-06-04 23:20:19.116878: W tensorflow/core/graph/graph_constructor.cc:1272] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2019-06-04 23:20:28.981485: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-06-04 23:20:28.989397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-06-04 23:20:29.241516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-06-04 23:20:29.248944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-06-04 23:20:29.250076: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0xb54907c0 executing computations on platform CUDA. Devices:\n",
      "2019-06-04 23:20:29.250105: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2019-06-04 23:20:29.250112: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2019-06-04 23:20:29.250125: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (2): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2019-06-04 23:20:29.253492: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "2019-06-04 23:20:29.254607: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1a929a730 executing computations on platform Host. Devices:\n",
      "2019-06-04 23:20:29.254639: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-06-04 23:20:29.254706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-04 23:20:29.254720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      \n",
      "Restoring from externals/data/model.max.ckpt\n",
      "Waiting a minute to allow all models to load.\n",
      "                  id            text pronoun  ...  b_offset b_coref             url\n",
      "0      development-1  Zoe Telford...     her  ...       207   False  http://en.w...\n",
      "1      development-2  He grew up ...     His  ...       251   False  http://en.w...\n",
      "2      development-3  He had been...     his  ...       246    True  http://en.w...\n",
      "3      development-4  The current...     his  ...       336    True  http://en.w...\n",
      "4      development-5  Her Santa F...     She  ...       294    True  http://en.w...\n",
      "...              ...             ...     ...  ...       ...     ...             ...\n",
      "1995  development...  Faye's thir...     her  ...       328    True  http://en.w...\n",
      "1996  development...  The plot of...     her  ...       215    True  http://en.w...\n",
      "1997  development...  Grant playe...     she  ...       266   False  http://en.w...\n",
      "1998  development...  The fashion...     She  ...       208   False  http://en.w...\n",
      "1999  development...  Watkins was...     her  ...       347    True  http://en.w...\n",
      "\n",
      "[2000 rows x 11 columns]\n",
      "                 id            text pronoun  ...  b_offset b_coref             url\n",
      "0      validation-1  He admitted...     him  ...       241   False  http://en.w...\n",
      "1      validation-2  Kathleen No...     She  ...       150    True  http://en.w...\n",
      "2      validation-3  When she re...     his  ...       406    True  http://en.w...\n",
      "3      validation-4  On 19 March...      he  ...       325   False  http://en.w...\n",
      "4      validation-5  By this tim...     she  ...       328    True  http://en.w...\n",
      "..              ...             ...     ...  ...       ...     ...             ...\n",
      "449  validation-450  He then agr...      He  ...       264   False  http://en.w...\n",
      "450  validation-451  Disgusted w...     she  ...       257   False  http://en.w...\n",
      "451  validation-452  She manipul...     she  ...       291    True  http://en.w...\n",
      "452  validation-453  On April 4,...     her  ...       294    True  http://en.w...\n",
      "453  validation-454  Pleasant ex...     him  ...       255   False  http://en.w...\n",
      "\n",
      "[454 rows x 11 columns]\n",
      "             id            text pronoun  ...  b_offset b_coref             url\n",
      "0        test-1  Upon their ...     His  ...       366    True  http://en.w...\n",
      "1        test-2  Between the...     him  ...       390   False  http://en.w...\n",
      "2        test-3  Though his ...      He  ...       295   False  http://en.w...\n",
      "3        test-4  At the tria...     his  ...       536    True  http://en.w...\n",
      "4        test-5  It is about...     his  ...       559   False  http://en.w...\n",
      "...         ...             ...     ...  ...       ...     ...             ...\n",
      "1995  test-1996  The sole ex...     She  ...       432   False  http://en.w...\n",
      "1996  test-1997  According t...     her  ...       404   False  http://en.w...\n",
      "1997  test-1998  In June 200...     She  ...       412   False  http://en.w...\n",
      "1998  test-1999  She was del...     she  ...       274   False  http://en.w...\n",
      "1999  test-2000  Meg and Vic...     her  ...       260   False  http://en.w...\n",
      "\n",
      "[2000 rows x 11 columns]\n",
      "            id            text pronoun  ...  b_offset b_coref             url\n",
      "0      test-16  Indeed, Bus...     his  ...       122   False  http://en.w...\n",
      "1      test-19  On May 17, ...     her  ...       293   False  http://en.w...\n",
      "2      test-24  Jones playe...      He  ...       319   False  http://en.w...\n",
      "3      test-41  However he ...     his  ...       213   False  http://en.w...\n",
      "4      test-46  When Liu Be...      he  ...       136   False  http://en.w...\n",
      "..         ...             ...     ...  ...       ...     ...             ...\n",
      "248  test-1968  Laura was t...     She  ...       189   False  http://en.w...\n",
      "249  test-1975  In the seve...     her  ...       209   False  http://en.w...\n",
      "250  test-1977  She also pl...     She  ...       105   False  http://en.w...\n",
      "251  test-1980  After Richa...     she  ...       392   False  http://en.w...\n",
      "252  test-1993  In a draft ...     her  ...       446   False  http://en.w...\n",
      "\n",
      "[253 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id            text  ... b_offset             url\r\n",
      "0      000075809a8...  For the U.S...  ...      160  http://en.w...\r\n",
      "1      0005d0f3b0a...  After this ...  ...      259  http://en.w...\r\n",
      "2      0007775c40b...  In the same...  ...      323  http://en.w...\r\n",
      "3      001194e3fe1...  Anita's so-...  ...      278  http://en.w...\r\n",
      "4      0014bb70852...  By March, s...  ...      336  http://en.w...\r\n",
      "...               ...             ...  ...      ...             ...\r\n",
      "12354  ffe42e03c80...  The trio re...  ...      183  http://en.w...\r\n",
      "12355  ffea29920b2...  In London, ...  ...      331  http://en.w...\r\n",
      "12356  ffec23d7150...  He has appe...  ...      248  http://en.w...\r\n",
      "12357  fff8142c3db...  Matthias Di...  ...       50  http://en.w...\r\n",
      "12358  fffc795862e...  Fay Templet...  ...      242  http://en.w...\r\n",
      "\r\n",
      "[12359 rows x 9 columns]\r\n",
      "Initializing Step InputReader\r\n",
      "Step InputReader initialized\r\n",
      "Initializing Step CorefExtractor\r\n",
      "Step CorefExtractor initialized\r\n",
      "Initializing Step PretrainedProref\r\n",
      "Step PretrainedProref initialized\r\n",
      "Initializing Step LabelSanitizer\r\n",
      "Step LabelSanitizer initialized\r\n",
      "Initializing Step CorefAnnotator\r\n",
      "Step CorefAnnotator initialized\r\n",
      "Initializing Step MentionsAnnotator\r\n",
      "Step MentionsAnnotator initialized\r\n",
      "Initializing Step PretrainedFeatures\r\n",
      "Step PretrainedFeatures initialized\r\n",
      "Initializing Step gather_step\r\n",
      "STEPPY WARNING: Step with name \"CorefExtractor\", already exist. Make sure that all Steps have unique name.\r\n",
      "Step gather_step initialized\r\n",
      "Initializing Step InputReader\r\n",
      "Step InputReader initialized\r\n",
      "Initializing Step CorefExtractor\r\n",
      "Step CorefExtractor initialized\r\n",
      "Initializing Step PretrainedProref\r\n",
      "Step PretrainedProref initialized\r\n",
      "Initializing Step LabelSanitizer\r\n",
      "Step LabelSanitizer initialized\r\n",
      "Initializing Step CorefAnnotator\r\n",
      "Step CorefAnnotator initialized\r\n",
      "Initializing Step MentionsAnnotator\r\n",
      "Step MentionsAnnotator initialized\r\n",
      "Initializing Step PretrainedFeatures\r\n",
      "Step PretrainedFeatures initialized\r\n",
      "Initializing Step gather_step\r\n",
      "STEPPY WARNING: Step with name \"CorefExtractor\", already exist. Make sure that all Steps have unique name.\r\n",
      "Step gather_step initialized\r\n",
      "Initializing Step InputReader\r\n",
      "Step InputReader initialized\r\n",
      "Initializing Step CorefExtractor\r\n",
      "Step CorefExtractor initialized\r\n",
      "Initializing Step PretrainedProref\r\n",
      "Step PretrainedProref initialized\r\n",
      "Initializing Step LabelSanitizer\r\n",
      "Step LabelSanitizer initialized\r\n",
      "Initializing Step CorefAnnotator\r\n",
      "Step CorefAnnotator initialized\r\n",
      "Initializing Step MentionsAnnotator\r\n",
      "Step MentionsAnnotator initialized\r\n",
      "Initializing Step PretrainedFeatures\r\n",
      "Step PretrainedFeatures initialized\r\n",
      "Initializing Step gather_step\r\n",
      "STEPPY WARNING: Step with name \"CorefExtractor\", already exist. Make sure that all Steps have unique name.\r\n",
      "Step gather_step initialized\r\n",
      "Initializing Step InputReader\r\n",
      "Step InputReader initialized\r\n",
      "Initializing Step CorefExtractor\r\n",
      "Step CorefExtractor initialized\r\n",
      "Initializing Step PretrainedProref\r\n",
      "Step PretrainedProref initialized\r\n",
      "Initializing Step LabelSanitizer\r\n",
      "Step LabelSanitizer initialized\r\n",
      "Initializing Step CorefAnnotator\r\n",
      "Step CorefAnnotator initialized\r\n",
      "Initializing Step MentionsAnnotator\r\n",
      "Step MentionsAnnotator initialized\r\n",
      "Initializing Step PretrainedFeatures\r\n",
      "Step PretrainedFeatures initialized\r\n",
      "Initializing Step gather_step\r\n",
      "STEPPY WARNING: Step with name \"CorefExtractor\", already exist. Make sure that all Steps have unique name.\r\n",
      "Step gather_step initialized\r\n",
      "{'edges': {('CorefAnnotator', 'MentionsAnnotator'),\r\n",
      "           ('CorefExtractor', 'CorefAnnotator'),\r\n",
      "           ('CorefExtractor', 'PretrainedProref'),\r\n",
      "           ('InputReader', 'gather_step'),\r\n",
      "           ('LabelSanitizer', 'gather_step'),\r\n",
      "           ('MentionsAnnotator', 'gather_step'),\r\n",
      "           ('PretrainedFeatures', 'gather_step'),\r\n",
      "           ('PretrainedProref', 'PretrainedFeatures'),\r\n",
      "           ('input', 'CorefAnnotator'),\r\n",
      "           ('input', 'CorefExtractor'),\r\n",
      "           ('input', 'InputReader'),\r\n",
      "           ('input', 'LabelSanitizer'),\r\n",
      "           ('input', 'PretrainedProref')},\r\n",
      " 'nodes': {'CorefAnnotator',\r\n",
      "           'CorefExtractor',\r\n",
      "           'InputReader',\r\n",
      "           'LabelSanitizer',\r\n",
      "           'MentionsAnnotator',\r\n",
      "           'PretrainedFeatures',\r\n",
      "           'PretrainedProref',\r\n",
      "           'gather_step',\r\n",
      "           'input'}}\r\n",
      "Transforming data to features.\r\n",
      "Step gather_step, working in \"train\" mode\r\n",
      "Step InputReader, working in \"train\" mode\r\n",
      "Step InputReader, adapting inputs\r\n",
      "Step InputReader, transforming...\r\n",
      "Step InputReader, transforming completed\r\n",
      "Step InputReader, transform completed\r\n",
      "Step LabelSanitizer, working in \"train\" mode\r\n",
      "Step LabelSanitizer, adapting inputs\r\n",
      "Step LabelSanitizer, transforming...\r\n",
      "      id                                            label\r\n",
      "0     14                                       Gracia (B)\r\n",
      "1     35                                 Lim Goh Tong (B)\r\n",
      "2     40                                          neither\r\n",
      "3    104                                Polly Simmons (B)\r\n",
      "4    120  neither (differentiate between person and role)\r\n",
      "5    140                                       Isabel (B)\r\n",
      "6    170                                          Day (B)\r\n",
      "7    171                                              (A)\r\n",
      "8    259                                              (B)\r\n",
      "9    260                                        Smith (A)\r\n",
      "10   290                                     Lewellyn (A)\r\n",
      "11   324                                   Umfraville (B)\r\n",
      "12   327                                          neither\r\n",
      "13   336                                          Rex (B)\r\n",
      "14   345                         neither (person vs role)\r\n",
      "15   347                                          neither\r\n",
      "16   376                                              (A)\r\n",
      "17   400                                              (A)\r\n",
      "18   429                                          neither\r\n",
      "19   443                                          neither\r\n",
      "20   456                                    Dan Brown (A)\r\n",
      "21   467                                          neither\r\n",
      "22   473                     neither (actor vs character)\r\n",
      "23   483                                       Huang (B) \r\n",
      "24   527                                     Cynthia (B) \r\n",
      "25   540                                Frances Black (B)\r\n",
      "26   547                                          neither\r\n",
      "27   565                                       Ragnar (A)\r\n",
      "28   577                                              (A)\r\n",
      "29   586                                  Leadbetter (A) \r\n",
      "..   ...                                              ...\r\n",
      "43  1050                                          neither\r\n",
      "44  1085                                          neither\r\n",
      "45  1160                                              (B)\r\n",
      "46  1191                                              (A)\r\n",
      "47  1203                                              (B)\r\n",
      "48  1215                                              (A)\r\n",
      "49  1258                                          neither\r\n",
      "50  1291                                          neither\r\n",
      "51  1337        (B)  (let’s see what the model does here)\r\n",
      "52  1368                                              (A)\r\n",
      "53  1415                                              (B)\r\n",
      "54  1421                                              (A)\r\n",
      "55  1454                                              (A)\r\n",
      "56  1484                                              (B)\r\n",
      "57  1512                                          neither\r\n",
      "58  1564                                              (B)\r\n",
      "59  1568                     neither (actor vs character)\r\n",
      "60  1569                     neither (actor vs character)\r\n",
      "61  1572                 neither (or A, definitely not B)\r\n",
      "62  1614                                              (A)\r\n",
      "63  1653                                              (B)\r\n",
      "64  1679                                              (B)\r\n",
      "65  1709                                              (B)\r\n",
      "66  1721                     neither (actor vs character)\r\n",
      "67  1744                                              (B)\r\n",
      "68  1793                                              (A)\r\n",
      "69  1804                     neither (actor vs character)\r\n",
      "70  1873                                              (A)\r\n",
      "71  1933                                              (A)\r\n",
      "72  1993                                              (B)\r\n",
      "\r\n",
      "[73 rows x 2 columns]\r\n",
      "Step LabelSanitizer, transforming completed\r\n",
      "Step LabelSanitizer, transform completed\r\n",
      "Step MentionsAnnotator, working in \"train\" mode\r\n",
      "Step CorefAnnotator, working in \"train\" mode\r\n",
      "Step CorefExtractor, working in \"train\" mode\r\n",
      "Step CorefExtractor, adapting inputs\r\n",
      "Step CorefExtractor, transforming...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:   44.9s\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:   52.0s\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed:  1.2min\n",
      "development-217, Tokens in parse tree and input sentence don't match.\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=8)]: Done 496 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=8)]: Done 529 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=8)]: Done 562 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=8)]: Done 597 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=8)]: Done 632 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=8)]: Done 669 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=8)]: Done 706 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=8)]: Done 745 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:  2.3min\n"
     ]
    }
   ],
   "source": [
    "!python run.py --train --kaggle --model=grep --language_model=bert-large-uncased --coref_models=url,allen,hug,lee --preprocess_train --preprocess_eval --verbose=1 --exp_dir=results/kaggle --test_path=data/test_stage_2.tsv --sub_sample_path=sample_submission_stage_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
